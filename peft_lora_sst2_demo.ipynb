{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "23500204",
      "metadata": {
        "id": "23500204"
      },
      "source": [
        "\n",
        "# Parameter-Efficient Fine-Tuning (PEFT) 실습\n",
        "**작성일**: 2025-07-18 by Yeongtak Oh (Credit : DSAIL LAb, SNU)\n",
        "\n",
        "**목표**:  \n",
        "- 사전 학습된 언어 모델에 대해 효율적인 파인튜닝 방법인 **LoRA (Low-Rank Adaptation)** 이해  \n",
        "- HuggingFace `peft` 라이브러리를 사용한 실습  \n",
        "- Colab에서 실행 가능한 코드 제공  \n",
        "- 예제 데이터셋: SST-2 (감정 분류)\n",
        "\n",
        "---\n",
        "\n",
        "> 본 노트북은 HD현대 실습을 위해 교육용 자료로서 준비되었으며, PEFT의 핵심 개념 및 실습을 포함합니다.  \n",
        "> **참고:** 이 노트북은 HuggingFace `transformers`, `datasets`, `peft` 라이브러리 사용을 기반으로 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351edb1d",
      "metadata": {
        "id": "351edb1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers datasets peft accelerate bitsandbytes --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets transformers"
      ],
      "metadata": {
        "id": "-RTdYczpZtfH"
      },
      "id": "-RTdYczpZtfH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b76dfc",
      "metadata": {
        "id": "63b76dfc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b337e454",
      "metadata": {
        "id": "b337e454"
      },
      "outputs": [],
      "source": [
        "# ✳️ 기본 모델 및 토크나이저 로드\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de1b221",
      "metadata": {
        "id": "5de1b221"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# ✳️ SST2 데이터셋 로드 및 전처리\n",
        "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
        "\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "encoded = dataset.map(tokenize_fn, batched=True)\n",
        "encoded = encoded.remove_columns([\"sentence\", \"idx\"])\n",
        "encoded.set_format(\"torch\")\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentences = list(dataset['train']['sentence'])"
      ],
      "metadata": {
        "id": "3Ds0GssFlF9a"
      },
      "id": "3Ds0GssFlF9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join([s.strip() for s in sentences])"
      ],
      "metadata": {
        "id": "i0LfaPgnlMTt"
      },
      "id": "i0LfaPgnlMTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db19685d",
      "metadata": {
        "id": "db19685d"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# ✳️ LoRA 설정 및 모델 래핑\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"]  # DistilBERT용. 또는 [\"query\", \"value\"] 등\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(base_model, peft_config)\n",
        "lora_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config"
      ],
      "metadata": {
        "id": "yLrExC-zlVW4"
      },
      "id": "yLrExC-zlVW4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9595ae",
      "metadata": {
        "id": "8a9595ae"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "# ✳️ 학습 인자 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_sst2\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded[\"train\"].shuffle(seed=42).select(range(1000)),  # 샘플 1000개 사용\n",
        "    eval_dataset=encoded[\"validation\"].select(range(300)),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args"
      ],
      "metadata": {
        "id": "usnRmNIplZYO"
      },
      "id": "usnRmNIplZYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.title('loss_plot')\n",
        "plt.xlabel('num of logging steps')\n",
        "plt.ylabel('training loss')\n",
        "plt.plot([trainer.state.log_history[i]['loss'] for i in range(len(trainer.state.log_history)-1)], label='LoRA loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "SDueAIvSkLbp"
      },
      "id": "SDueAIvSkLbp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bde8f7d",
      "metadata": {
        "id": "8bde8f7d"
      },
      "outputs": [],
      "source": [
        "# ✳️ 평가 및 저장\n",
        "metrics = trainer.evaluate()\n",
        "print(\"\\n📊 평가 결과:\", metrics)\n",
        "\n",
        "# LoRA adapter만 저장\n",
        "lora_model.save_pretrained(\"./lora_adapter_sst2\")\n",
        "print(\"💾 LoRA adapter 저장 완료: ./lora_adapter_sst2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c2c9318",
      "metadata": {
        "id": "9c2c9318"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 📌 LoRA란?\n",
        "\n",
        "LoRA는 **Low-Rank Adaptation of Large Language Models**의 줄임말로,  \n",
        "사전학습된 모델의 파라미터는 **동결(freeze)** 하고,  \n",
        "일부 층에 대해 **low-rank 행렬을 추가 학습**함으로써  \n",
        "**매우 적은 수의 학습 가능한 파라미터**로도 fine-tuning이 가능하게 하는 기법입니다.\n",
        "\n",
        "> 📎 참고 문헌: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 실습 핵심 포인트\n",
        "\n",
        "- `AutoModelForSequenceClassification`: 분류용 pre-trained 모델 사용  \n",
        "- `peft.LoraConfig`: 파인튜닝할 레이어 설정 (dropout, rank 등)  \n",
        "- `get_peft_model`: 기존 모델을 LoRA 적용 가능한 형태로 래핑  \n",
        "- **trainable parameters**만 출력 확인 → 기존 모델 대비 약 0.1~1% 수준  \n",
        "- SST-2 감정 분류 데이터셋 사용 (긍정/부정)\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 결과 확인 예시\n",
        "\n",
        "- 정확도(accuracy), 손실(loss) 등 기본 평가 지표 출력  \n",
        "- `save_pretrained()` 호출 시 full model이 아니라 LoRA adapter만 저장됨  \n",
        "\n",
        "---  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bddc0b32",
      "metadata": {
        "id": "bddc0b32"
      },
      "source": [
        "\n",
        "## 📝 퀴즈 및 복습 질문\n",
        "\n",
        "1. LoRA의 핵심 아이디어는 무엇인가요? 어떤 장점이 있나요?  \n",
        "2. LoRA를 적용하기 위해 어떤 구성요소들이 필요한가요?  \n",
        "3. Full fine-tuning과 비교해 PEFT의 제한사항은 무엇이 있을까요?\n",
        "\n",
        "> 아래 셀에 자유롭게 작성해 보세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27c185b",
      "metadata": {
        "id": "e27c185b"
      },
      "outputs": [],
      "source": [
        "# 여기에 작성하세요"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}