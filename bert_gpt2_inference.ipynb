{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# BERT, GPT-2 실습 노트북\n","\n","**작성일**: 2025-07-19 by Youngwoo Kimh (Credit : DSAIL Lab, SNU)\n","\n","**목표**:  \n","- BERT 및 GPT-2의 구조 이해\n","- BERT의 학습 목표 (MLM, NSP) 이해\n","\n","---\n","> 본 노트북은 HD현대 실습을 위해 교육용 자료로서 준비되었으며, PyTorch와 HuggingFace Transformers 라이브러리를 사용합니다."],"metadata":{"id":"bvqzlEg86WUD"}},{"cell_type":"markdown","source":["### 1. BERT"],"metadata":{"id":"49WyX3j6TZ0x"}},{"cell_type":"markdown","source":["MLM"],"metadata":{"id":"6e60OjDrnmjU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFRRATqBJidl"},"outputs":[],"source":["from transformers import pipeline, BertTokenizer, BertForMaskedLM\n","import torch, os\n","print('Transformers version:', __import__('transformers').__version__)"]},{"cell_type":"code","source":["# load tokenizer and model\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')"],"metadata":{"id":"LgZ2P4BxJuJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model)\n","print(\"\\nEmbedding matrices:\")\n","\n","# Token embeddings\n","print(\"Token embeddings  :\", model.bert.embeddings.word_embeddings.weight.shape)\n","\n","# Position embeddings\n","print(\"Position embeddings:\", model.bert.embeddings.position_embeddings.weight.shape)\n","\n","# Segment embeddings\n","print(\"Segment(Type) embeddings:\", model.bert.embeddings.token_type_embeddings.weight.shape)"],"metadata":{"id":"GvOgxEnHYZvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_top_k(prompt, k = 5):\n","\n","    # tokenize sentence\n","    inputs = tokenizer(prompt, return_tensors='pt')\n","    mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n","\n","    # inference\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","\n","    # probability for each word in the dictionary\n","    predicted_token_id = logits[0, mask_idx]\n","\n","    top_k_tokens = torch.topk(predicted_token_id, k)\n","    top_k_probs = torch.softmax(predicted_token_id, dim=-1)\n","\n","    print('Top', k, 'predicted tokens:')\n","    for i, token_id in enumerate(top_k_tokens.indices[0]):\n","        token = tokenizer.decode([token_id])\n","        probability = top_k_probs[0, token_id].item()\n","        print(f'{i+1}: {token} ({probability:.4f})')"],"metadata":{"id":"vXE2uN7dNUNk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'The capital of France is [MASK].'\n","\n","inputs = tokenizer(text, return_tensors='pt')\n","\n","mask_idx = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n","\n","# tokenized words\n","print(inputs['input_ids'])\n","\n","# token type\n","print(inputs['token_type_ids'])\n","\n","# attention mask\n","print(inputs['attention_mask'])\n","\n","# index of the [MASK] token\n","print(mask_idx)"],"metadata":{"id":"0upFIg3BK_oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inference [MASK]\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","predicted_token_id = logits[0, mask_idx]\n","\n","k = 5\n","top_k_tokens = torch.topk(predicted_token_id, k)\n","top_k_probs = torch.softmax(predicted_token_id, dim=-1)\n","\n","print('Top', k, 'predicted tokens:')\n","for i, token_id in enumerate(top_k_tokens.indices[0]):\n","    token = tokenizer.decode([token_id])\n","    probability = top_k_probs[0, token_id].item()\n","    print(f'{i+1}: {token} ({probability:.4f})')"],"metadata":{"id":"3GPIeLWWKWVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'I went to [MASK] yesterday.'\n","show_top_k(text)"],"metadata":{"id":"DHYpMgAiNoRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'The most important thing while bouldering is [MASK].'\n","show_top_k(text)"],"metadata":{"id":"OQ2HaVKJNtu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = '' # any sentence you want. enter [MASK] that you want to mask.\n","show_top_k(text)"],"metadata":{"id":"vmwRdPaTtK_U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NSP"],"metadata":{"id":"FWDFHPeznoxF"}},{"cell_type":"code","source":["from transformers import BertForNextSentencePrediction\n","\n","# load model for next sentence prediction\n","nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n","\n","# positive pair\n","sent_a = \"The Eiffel Tower is located in Paris.\"\n","sent_b = \"It is one of the most famous landmarks in the world.\"\n","encoding = tokenizer(sent_a, sent_b, return_tensors='pt')"],"metadata":{"id":"P_eH1O6wOlIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(nsp_model)\n","print(encoding['input_ids'])\n","print(encoding['token_type_ids'])\n","print(encoding['attention_mask'])\n","\n","# token number for [CLS] and [SEP]\n","print(tokenizer.cls_token, ':', tokenizer.cls_token_id)\n","print(tokenizer.sep_token, ':' , tokenizer.sep_token_id)"],"metadata":{"id":"8M67MWKxO2Wb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inference\n","logits = nsp_model(**encoding).logits\n","\n","# probability that sent_b is the next sentence of sent_a\n","prob = torch.softmax(logits, dim=1)\n","print(\"IsNext prob (positive):\", prob)\n","print(torch.argmax(prob, dim=1).item())"],"metadata":{"id":"IdAFmKTzPMJ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_next_sentence(sent_1, sent_2):\n","    encoding = tokenizer(sent_1, sent_2, return_tensors='pt')\n","    logits = nsp_model(**encoding).logits\n","    prob = torch.softmax(logits, dim=1)\n","    print(\"IsNext prob (positive):\", prob)\n","    print(torch.argmax(prob, dim=1).item())"],"metadata":{"id":"Fu8bcgX-Pbjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent_c = \"Deep learning models require large datasets.\"\n","is_next_sentence(sent_a, sent_c)"],"metadata":{"id":"uYDRru92Pmqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent_a = '' # fill your own sentence\n","sent_b = ''\n","is_next_sentence(sent_a, sent_b)"],"metadata":{"id":"RdIsVDorn9qm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. GPT-2"],"metadata":{"id":"ACHOwL-x6Syi"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","import pandas as pd"],"metadata":{"id":"icpgJ4mQiLWA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load tokenizer and model for gpt-2\n","gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n","print(gpt2_model)"],"metadata":{"id":"RJCLOQuKiM4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize input sentence\n","sentence = 'In the future, artificial intelligence will'\n","input_ids = gpt2_tokenizer.encode(sentence, return_tensors='pt')\n","print(input_ids)"],"metadata":{"id":"Dm-My1a-iOcv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate the next word after the input sentence\n","generated_ids = gpt2_model.generate(\n","    input_ids,\n","    max_length=50,        # total tokens (prompt + generated)\n","    do_sample=True,       # switch on sampling\n","    top_k=50,             # top‑k sampling\n","    top_p=0.95,           # nucleus sampling\n","    temperature=0.8,      # softness of probabilities\n","    eos_token_id=gpt2_tokenizer.eos_token_id,\n",")\n","\n","generated_text = gpt2_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","print(generated_text)"],"metadata":{"id":"w3wevSASiPzw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Attention Mask comparison"],"metadata":{"id":"3w5cmug56aEi"}},{"cell_type":"code","source":["text = \"The quick brown fox jumps.\"\n","bert_tok = tokenizer\n","gpt2_tok = gpt2_tokenizer\n","\n","bert_ids = bert_tok(text, return_tensors=\"pt\")\n","gpt2_ids = gpt2_tok(text, return_tensors=\"pt\")\n","\n","bert = model\n","gpt2 = gpt2_model\n","\n","# attention mask for decoder-only transformer based gpt-2\n","causal_mask = gpt2.transformer.h[0].attn.bias.float()\n","print(\"GPT-2 attention mask:\")\n","print(causal_mask)\n"],"metadata":{"id":"UOe2P-c81Fd6"},"execution_count":null,"outputs":[]}]}